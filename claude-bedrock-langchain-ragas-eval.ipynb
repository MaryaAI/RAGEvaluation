{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluate Retrieval Augmented Generation with Anthropic Claude 3, Amazon Bedrock, Langchain and RAGAS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "\n",
    "In this notebook we will show you how to use Langchain, Anthropic Claude 3, Knowledge base for Amazon Bedrock and RAGAS to evaluate response of a Retrieval Augmented Generation (RAG) solution\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Python 3.10\n",
    "\n",
    "⚠  For this lab we need to run the notebook based on a Python 3.10 runtime. ⚠\n",
    "\n",
    "\n",
    "## Installation\n",
    "\n",
    "To run this notebook you would need to install dependencies - boto3, botocore and langchain."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install boto3\n",
    "# !pip install awscli"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# %pip install --upgrade pip\n",
    "# %pip install boto3 --force-reinstall --quiet\n",
    "# %pip install botocore --force-reinstall --quiet\n",
    "# %pip install langchain>0.1 --force-reinstall --quiet\n",
    "# %pip install ragas>0.1 --force-reinstall --quiet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Kernel Restart\n",
    "\n",
    "Restart the kernel with the updated packages that are installed through the dependencies above"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# restart kernel\n",
    "from IPython.core.display import HTML\n",
    "HTML(\"<script>Jupyter.notebook.kernel.restart()</script>\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup \n",
    "\n",
    "Import the necessary libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "import sys\n",
    "import boto3\n",
    "import botocore\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_community.chat_models.bedrock import BedrockChat\n",
    "from botocore.client import Config\n",
    "from langchain_community.retrievers import AmazonKnowledgeBasesRetriever\n",
    "from langchain.schema.runnable import RunnablePassthrough\n",
    "from datasets import Dataset\n",
    "from langchain.embeddings import BedrockEmbeddings\n",
    "import pandas as pd\n",
    "from langchain.chains import RetrievalQA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialization\n",
    "\n",
    "Initiate Bedrock Runtime and BedrockChat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "bedrock_config = Config(connect_timeout=120, read_timeout=120, retries={'max_attempts': 0})\n",
    "bedrock_client = boto3.client('bedrock-runtime')\n",
    "\n",
    "modelId = 'anthropic.claude-3-sonnet-20240229-v1:0' # change this to use a different version from the model provider\n",
    "\n",
    "llm = BedrockChat(model_id=modelId, client=bedrock_client)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Retrieval\n",
    "\n",
    "Enter Knowledge Base id and retrieve relevant documents from Knowledge Base for Amazon Bedrock"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "retriever = AmazonKnowledgeBasesRetriever(\n",
    "    knowledge_base_id=\"\", # enter knowledge base id here\n",
    "    retrieval_config={\"vectorSearchConfiguration\": {\"numberOfResults\": 4}},\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Knowledge Base ID: UNWIC3XUHD\n",
      "Retrieval Config: {'vectorSearchConfiguration': {'numberOfResults': 4}}\n"
     ]
    }
   ],
   "source": [
    "# Verify the knowledge base and retrieval configuration\n",
    "print(f\"Knowledge Base ID: {retriever.knowledge_base_id}\")\n",
    "print(f\"Retrieval Config: {retriever.retrieval_config.dict()}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Invocation and Response Generation using RetrievalQA chain\n",
    "\n",
    "Invoke the model and visualize the response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Based on the context provided, this appears to be a paper about transformer models for machine translation tasks. Some key details that indicate this:\n",
      "\n",
      "- It mentions results on the WMT 2014 English-to-German and English-to-French machine translation tasks, reporting new state-of-the-art BLEU scores using \"big transformer models\".\n",
      "\n",
      "- It discusses training details like dropout rates, averaging checkpoints, and training times/costs for the transformer models on these translation tasks.\n",
      "\n",
      "- Many of the referenced papers seem to be about sequence-to-sequence models, neural machine translation, language modeling, etc.\n",
      "\n",
      "So while I don't have the full context of the paper, the provided text strongly suggests the main topic is transformer neural network architectures applied to machine translation between languages like English, German and French.\n"
     ]
    }
   ],
   "source": [
    "query = \"what is the main topic in the paper?\"\n",
    "\n",
    "qa_chain = RetrievalQA.from_chain_type(\n",
    "    llm=llm, retriever=retriever, return_source_documents=True\n",
    ")\n",
    "\n",
    "response = qa_chain.invoke(query)\n",
    "print(response[\"result\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparing the Evaluation Data\n",
    "\n",
    "As RAGAS aims to be a reference-free evaluation framework, the required preparations of the evaluation dataset are minimal. You will need to prepare `question` and `ground_truths` pairs from which you can prepare the remaining information through inference as shown below. If you are not interested in the `context_recall` metric, you don’t need to provide the `ground_truths` information. In this case, all you need to prepare are the questions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\rahaf\\AppData\\Local\\Temp\\ipykernel_41864\\4177382640.py:15: LangChainDeprecationWarning: The method `BaseRetriever.get_relevant_documents` was deprecated in langchain-core 0.1.46 and will be removed in 1.0. Use :meth:`~invoke` instead.\n",
      "  contexts.append([docs.page_content for docs in retriever.get_relevant_documents(query)])\n"
     ]
    }
   ],
   "source": [
    "questions = [\"What is the name of the paper\", \n",
    "             \"what are the name of the authers?\",\n",
    "             \"what is the purpose of the paper?\"\n",
    "            ]\n",
    "ground_truths = [[\"Attention Is All You Need\"],\n",
    "                [\"Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Łukasz Kaiser, and Illia Polosukhin.\"],\n",
    "                [\"The purpose of the paper 'Attention Is All You Need' is to introduce the Transformer architecture, a novel model based entirely on self-attention mechanisms, aimed at improving efficiency and scalability in sequence transduction tasks such as machine translation, by eliminating the need for recurrent and convolutional networks\"]\n",
    "                ]\n",
    "\n",
    "answers = []\n",
    "contexts = []\n",
    "\n",
    "for query in questions:\n",
    "  answers.append(qa_chain.invoke(query)[\"result\"])\n",
    "  contexts.append([docs.page_content for docs in retriever.get_relevant_documents(query)])\n",
    "\n",
    "# To dict\n",
    "data = {\n",
    "    \"question\": questions,\n",
    "    \"answer\": answers,\n",
    "    \"contexts\": contexts,\n",
    "    \"ground_truths\": ground_truths\n",
    "}\n",
    "\n",
    "# Convert dict to dataset\n",
    "dataset = Dataset.from_dict(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluating the RAG application\n",
    "\n",
    "First, import all the metrics you want to use from `ragas.metrics`. Then, you can use the `evaluate()` function and simply pass in the relevant metrics and the prepared dataset. Below is a brief description of the metrics\n",
    "\n",
    "* **Faithfulness**: This measures the factual consistency of the generated answer against the given context. It is calculated from answer and retrieved context. The answer is scaled to (0,1) range. Higher the better.\n",
    "* **Answer Relevance**: The evaluation metric, Answer Relevancy, focuses on assessing how pertinent the generated answer is to the given prompt. A lower score is assigned to answers that are incomplete or contain redundant information and higher scores indicate better relevancy. This metric is computed using the question, the context and the answer. Please note, that eventhough in practice the score will range between 0 and 1 most of the time, this is not mathematically guaranteed, due to the nature of the cosine similarity ranging from -1 to 1.\n",
    "* **Context Precision**: Context Precision is a metric that evaluates whether all of the ground-truth relevant items present in the contexts are ranked higher or not. Ideally all the relevant chunks must appear at the top ranks. This metric is computed using the question, ground_truth and the contexts, with values ranging between 0 and 1, where higher scores indicate better precision.\n",
    "* **Context Recall**: Context recall measures the extent to which the retrieved context aligns with the annotated answer, treated as the ground truth. It is computed based on the ground truth and the retrieved context, and the values range between 0 and 1, with higher values indicating better performance.\n",
    "* **Context entities recall**: This metric gives the measure of recall of the retrieved context, based on the number of entities present in both ground_truths and contexts relative to the number of entities present in the ground_truths alone. Simply put, it is a measure of what fraction of entities are recalled from ground_truths. This metric is useful in fact-based use cases like tourism help desk, historical QA, etc. This metric can help evaluate the retrieval mechanism for entities, based on comparison with entities present in ground_truths, because in cases where entities matter, we need the contexts which cover them.\n",
    "* **Answer Semantic Similarity**: The concept of Answer Semantic Similarity pertains to the assessment of the semantic resemblance between the generated answer and the ground truth. This evaluation is based on the ground truth and the answer, with values falling within the range of 0 to 1. A higher score signifies a better alignment between the generated answer and the ground truth.\n",
    "* **Answer Correctness**: The assessment of Answer Correctness involves gauging the accuracy of the generated answer when compared to the ground truth. This evaluation relies on the ground truth and the answer, with scores ranging from 0 to 1. A higher score indicates a closer alignment between the generated answer and the ground truth, signifying better correctness. Answer correctness encompasses two critical aspects: semantic similarity between the generated answer and the ground truth, as well as factual similarity. These aspects are combined using a weighted scheme to formulate the answer correctness score. Users also have the option to employ a ‘threshold’ value to round the resulting score to binary, if desired.\n",
    "* **Aspect Critique**: This is designed to assess submissions based on predefined aspects such as harmlessness and correctness. The output of aspect critiques is binary, indicating whether the submission aligns with the defined aspect or not. This evaluation is performed using the ‘answer’ as input."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6d345f4c7ad143d98fcc66e0980b9b0f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluating:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user_input</th>\n",
       "      <th>retrieved_contexts</th>\n",
       "      <th>response</th>\n",
       "      <th>faithfulness</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>What is the name of the paper</td>\n",
       "      <td>[ACL, June 2006.     [27] Ankur Parikh, Oscar Täckström, Dipanjan Das, and Jakob Uszkoreit. A decomposable attention model. In Empirical Methods in Natural Language Processing, 2016.     [28] Romain Paulus, Caiming Xiong, and Richard Socher. A deep reinforced model for abstractive summarization. arXiv preprint arXiv:1705.04304, 2017.     [29] Slav Petrov, Leon Barrett, Romain Thibaux, and Dan Klein. Learning accurate, compact, and interpretable tree annotation. In Proceedings of the 21st International Conference on Computational Linguistics and 44th Annual Meeting of the ACL, pages 433–440. ACL, July 2006.     [30] Ofir Press and Lior Wolf. Using the output embedding to improve language models. arXiv preprint arXiv:1608.05859, 2016.     [31] Rico Sennrich, Barry Haddow, and Alexandra B...</td>\n",
       "      <td>Unfortunately, the given context does not seem to explicitly mention the name of a specific paper. It appears to be a list of references/citations from a research paper or document, but the title of that main paper is not provided.</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>what are the name of the authers?</td>\n",
       "      <td>[Niki designed, implemented, tuned and evaluated countless model variants in our original codebase and tensor2tensor. Llion also experimented with novel model variants, was responsible for our initial codebase, and efficient inference and visualizations. Lukasz and Aidan spent countless long days designing various parts of and implementing tensor2tensor, replacing our earlier codebase, greatly improving results and massively accelerating our research.     †Work performed while at Google Brain. ‡Work performed while at Google Research.     31st Conference on Neural Information Processing Systems (NIPS 2017), Long Beach, CA, USA.     ar X     iv :1     70 6.     03 76     2v 7      [ cs     .C L     ] 2      A ug      2 02     31 Introduction     Recurrent neural networks, long short-ter...</td>\n",
       "      <td>The authors are not explicitly listed, but based on the context provided, some names that are mentioned are:\\n\\n- Niki\\n- Llion\\n- Lukasz\\n- Aidan\\n\\nIt seems Niki, Llion, Lukasz and Aidan worked on designing, implementing and experimenting with different model variants for this work while at Google Brain and Google Research. However, their full names or roles are not clearly specified in the given context.</td>\n",
       "      <td>0.833333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>what is the purpose of the paper?</td>\n",
       "      <td>[We plan to extend the Transformer to problems involving input and output modalities other than text and to investigate local, restricted attention mechanisms to efficiently handle large inputs and outputs such as images, audio and video. Making generation less sequential is another research goals of ours.     The code we used to train and evaluate our models is available at https://github.com/ tensorflow/tensor2tensor.     Acknowledgements We are grateful to Nal Kalchbrenner and Stephan Gouws for their fruitful comments, corrections and inspiration.     References [1] Jimmy Lei Ba, Jamie Ryan Kiros, and Geoffrey E Hinton. Layer normalization. arXiv preprint     arXiv:1607.06450, 2016.     [2] Dzmitry Bahdanau, Kyunghyun Cho, and Yoshua Bengio. Neural machine translation by jointly lea...</td>\n",
       "      <td>The paper introduces the Transformer, a new neural network architecture for sequence modeling tasks like machine translation that does not use recurrent connections. Instead, it relies entirely on attention mechanisms to capture long-range dependencies within the input and output sequences. The key motivations seem to be achieving better parallelization than recurrent models, while benefiting from the ability of attention to directly model relationships between distant elements in the sequences.</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                          user_input  \\\n",
       "0      What is the name of the paper   \n",
       "1  what are the name of the authers?   \n",
       "2  what is the purpose of the paper?   \n",
       "\n",
       "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                retrieved_contexts  \\\n",
       "0  [ACL, June 2006.     [27] Ankur Parikh, Oscar Täckström, Dipanjan Das, and Jakob Uszkoreit. A decomposable attention model. In Empirical Methods in Natural Language Processing, 2016.     [28] Romain Paulus, Caiming Xiong, and Richard Socher. A deep reinforced model for abstractive summarization. arXiv preprint arXiv:1705.04304, 2017.     [29] Slav Petrov, Leon Barrett, Romain Thibaux, and Dan Klein. Learning accurate, compact, and interpretable tree annotation. In Proceedings of the 21st International Conference on Computational Linguistics and 44th Annual Meeting of the ACL, pages 433–440. ACL, July 2006.     [30] Ofir Press and Lior Wolf. Using the output embedding to improve language models. arXiv preprint arXiv:1608.05859, 2016.     [31] Rico Sennrich, Barry Haddow, and Alexandra B...   \n",
       "1  [Niki designed, implemented, tuned and evaluated countless model variants in our original codebase and tensor2tensor. Llion also experimented with novel model variants, was responsible for our initial codebase, and efficient inference and visualizations. Lukasz and Aidan spent countless long days designing various parts of and implementing tensor2tensor, replacing our earlier codebase, greatly improving results and massively accelerating our research.     †Work performed while at Google Brain. ‡Work performed while at Google Research.     31st Conference on Neural Information Processing Systems (NIPS 2017), Long Beach, CA, USA.     ar X     iv :1     70 6.     03 76     2v 7      [ cs     .C L     ] 2      A ug      2 02     31 Introduction     Recurrent neural networks, long short-ter...   \n",
       "2  [We plan to extend the Transformer to problems involving input and output modalities other than text and to investigate local, restricted attention mechanisms to efficiently handle large inputs and outputs such as images, audio and video. Making generation less sequential is another research goals of ours.     The code we used to train and evaluate our models is available at https://github.com/ tensorflow/tensor2tensor.     Acknowledgements We are grateful to Nal Kalchbrenner and Stephan Gouws for their fruitful comments, corrections and inspiration.     References [1] Jimmy Lei Ba, Jamie Ryan Kiros, and Geoffrey E Hinton. Layer normalization. arXiv preprint     arXiv:1607.06450, 2016.     [2] Dzmitry Bahdanau, Kyunghyun Cho, and Yoshua Bengio. Neural machine translation by jointly lea...   \n",
       "\n",
       "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                               response  \\\n",
       "0                                                                                                                                                                                                                                                                               Unfortunately, the given context does not seem to explicitly mention the name of a specific paper. It appears to be a list of references/citations from a research paper or document, but the title of that main paper is not provided.   \n",
       "1                                                                                            The authors are not explicitly listed, but based on the context provided, some names that are mentioned are:\\n\\n- Niki\\n- Llion\\n- Lukasz\\n- Aidan\\n\\nIt seems Niki, Llion, Lukasz and Aidan worked on designing, implementing and experimenting with different model variants for this work while at Google Brain and Google Research. However, their full names or roles are not clearly specified in the given context.   \n",
       "2  The paper introduces the Transformer, a new neural network architecture for sequence modeling tasks like machine translation that does not use recurrent connections. Instead, it relies entirely on attention mechanisms to capture long-range dependencies within the input and output sequences. The key motivations seem to be achieving better parallelization than recurrent models, while benefiting from the ability of attention to directly model relationships between distant elements in the sequences.   \n",
       "\n",
       "   faithfulness  \n",
       "0      1.000000  \n",
       "1      0.833333  \n",
       "2      1.000000  "
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from ragas import evaluate\n",
    "from ragas.metrics import (\n",
    "    faithfulness,\n",
    "    answer_relevancy,\n",
    "    context_recall,\n",
    "    context_precision,\n",
    "    context_entity_recall,\n",
    "    answer_similarity,\n",
    "    answer_correctness\n",
    ")\n",
    "\n",
    "\n",
    "#specify the metrics here\n",
    "metrics = [\n",
    "        faithfulness,       \n",
    "#         context_precision,\n",
    "#         context_recall\n",
    "#         context_entity_recall,\n",
    "#         answer_similarity,\n",
    "#         answer_correctness,\n",
    "#         harmfulness, \n",
    "#         maliciousness, \n",
    "#         coherence, \n",
    "#         correctness, \n",
    "#         conciseness\n",
    "    ]\n",
    "\n",
    "#You can also choose a different model for evaluation\n",
    "llm_for_evaluation = BedrockChat(model_id=modelId, client=bedrock_client)\n",
    "bedrock_embeddings = BedrockEmbeddings(model_id=\"amazon.titan-embed-text-v1\",client=bedrock_client)\n",
    "\n",
    "result = evaluate(\n",
    "    dataset = dataset, \n",
    "    metrics=metrics,\n",
    "    llm=llm_for_evaluation,\n",
    "    embeddings=bedrock_embeddings,\n",
    ")\n",
    "\n",
    "df = result.to_pandas()\n",
    "\n",
    "\n",
    "pd.options.display.max_colwidth = 800\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e897081d11894b4eaab55d3a60d68b65",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluating:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user_input</th>\n",
       "      <th>retrieved_contexts</th>\n",
       "      <th>response</th>\n",
       "      <th>faithfulness</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>What is the name of the paper</td>\n",
       "      <td>[ACL, June 2006.     [27] Ankur Parikh, Oscar Täckström, Dipanjan Das, and Jakob Uszkoreit. A decomposable attention model. In Empirical Methods in Natural Language Processing, 2016.     [28] Romain Paulus, Caiming Xiong, and Richard Socher. A deep reinforced model for abstractive summarization. arXiv preprint arXiv:1705.04304, 2017.     [29] Slav Petrov, Leon Barrett, Romain Thibaux, and Dan Klein. Learning accurate, compact, and interpretable tree annotation. In Proceedings of the 21st International Conference on Computational Linguistics and 44th Annual Meeting of the ACL, pages 433–440. ACL, July 2006.     [30] Ofir Press and Lior Wolf. Using the output embedding to improve language models. arXiv preprint arXiv:1608.05859, 2016.     [31] Rico Sennrich, Barry Haddow, and Alexandra B...</td>\n",
       "      <td>Unfortunately, the given context does not seem to explicitly mention the name of a specific paper. It appears to be a list of references/citations from a research paper or document, but the title of that main paper is not provided.</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>what are the name of the authers?</td>\n",
       "      <td>[Niki designed, implemented, tuned and evaluated countless model variants in our original codebase and tensor2tensor. Llion also experimented with novel model variants, was responsible for our initial codebase, and efficient inference and visualizations. Lukasz and Aidan spent countless long days designing various parts of and implementing tensor2tensor, replacing our earlier codebase, greatly improving results and massively accelerating our research.     †Work performed while at Google Brain. ‡Work performed while at Google Research.     31st Conference on Neural Information Processing Systems (NIPS 2017), Long Beach, CA, USA.     ar X     iv :1     70 6.     03 76     2v 7      [ cs     .C L     ] 2      A ug      2 02     31 Introduction     Recurrent neural networks, long short-ter...</td>\n",
       "      <td>The authors are not explicitly listed, but based on the context provided, some names that are mentioned are:\\n\\n- Niki\\n- Llion\\n- Lukasz\\n- Aidan\\n\\nIt seems Niki, Llion, Lukasz and Aidan worked on designing, implementing and experimenting with different model variants for this work while at Google Brain and Google Research. However, their full names or roles are not clearly specified in the given context.</td>\n",
       "      <td>0.833333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>what is the purpose of the paper?</td>\n",
       "      <td>[We plan to extend the Transformer to problems involving input and output modalities other than text and to investigate local, restricted attention mechanisms to efficiently handle large inputs and outputs such as images, audio and video. Making generation less sequential is another research goals of ours.     The code we used to train and evaluate our models is available at https://github.com/ tensorflow/tensor2tensor.     Acknowledgements We are grateful to Nal Kalchbrenner and Stephan Gouws for their fruitful comments, corrections and inspiration.     References [1] Jimmy Lei Ba, Jamie Ryan Kiros, and Geoffrey E Hinton. Layer normalization. arXiv preprint     arXiv:1607.06450, 2016.     [2] Dzmitry Bahdanau, Kyunghyun Cho, and Yoshua Bengio. Neural machine translation by jointly lea...</td>\n",
       "      <td>The paper introduces the Transformer, a new neural network architecture for sequence modeling tasks like machine translation that does not use recurrent connections. Instead, it relies entirely on attention mechanisms to capture long-range dependencies within the input and output sequences. The key motivations seem to be achieving better parallelization than recurrent models, while benefiting from the ability of attention to directly model relationships between distant elements in the sequences.</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                          user_input  \\\n",
       "0      What is the name of the paper   \n",
       "1  what are the name of the authers?   \n",
       "2  what is the purpose of the paper?   \n",
       "\n",
       "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                retrieved_contexts  \\\n",
       "0  [ACL, June 2006.     [27] Ankur Parikh, Oscar Täckström, Dipanjan Das, and Jakob Uszkoreit. A decomposable attention model. In Empirical Methods in Natural Language Processing, 2016.     [28] Romain Paulus, Caiming Xiong, and Richard Socher. A deep reinforced model for abstractive summarization. arXiv preprint arXiv:1705.04304, 2017.     [29] Slav Petrov, Leon Barrett, Romain Thibaux, and Dan Klein. Learning accurate, compact, and interpretable tree annotation. In Proceedings of the 21st International Conference on Computational Linguistics and 44th Annual Meeting of the ACL, pages 433–440. ACL, July 2006.     [30] Ofir Press and Lior Wolf. Using the output embedding to improve language models. arXiv preprint arXiv:1608.05859, 2016.     [31] Rico Sennrich, Barry Haddow, and Alexandra B...   \n",
       "1  [Niki designed, implemented, tuned and evaluated countless model variants in our original codebase and tensor2tensor. Llion also experimented with novel model variants, was responsible for our initial codebase, and efficient inference and visualizations. Lukasz and Aidan spent countless long days designing various parts of and implementing tensor2tensor, replacing our earlier codebase, greatly improving results and massively accelerating our research.     †Work performed while at Google Brain. ‡Work performed while at Google Research.     31st Conference on Neural Information Processing Systems (NIPS 2017), Long Beach, CA, USA.     ar X     iv :1     70 6.     03 76     2v 7      [ cs     .C L     ] 2      A ug      2 02     31 Introduction     Recurrent neural networks, long short-ter...   \n",
       "2  [We plan to extend the Transformer to problems involving input and output modalities other than text and to investigate local, restricted attention mechanisms to efficiently handle large inputs and outputs such as images, audio and video. Making generation less sequential is another research goals of ours.     The code we used to train and evaluate our models is available at https://github.com/ tensorflow/tensor2tensor.     Acknowledgements We are grateful to Nal Kalchbrenner and Stephan Gouws for their fruitful comments, corrections and inspiration.     References [1] Jimmy Lei Ba, Jamie Ryan Kiros, and Geoffrey E Hinton. Layer normalization. arXiv preprint     arXiv:1607.06450, 2016.     [2] Dzmitry Bahdanau, Kyunghyun Cho, and Yoshua Bengio. Neural machine translation by jointly lea...   \n",
       "\n",
       "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                               response  \\\n",
       "0                                                                                                                                                                                                                                                                               Unfortunately, the given context does not seem to explicitly mention the name of a specific paper. It appears to be a list of references/citations from a research paper or document, but the title of that main paper is not provided.   \n",
       "1                                                                                            The authors are not explicitly listed, but based on the context provided, some names that are mentioned are:\\n\\n- Niki\\n- Llion\\n- Lukasz\\n- Aidan\\n\\nIt seems Niki, Llion, Lukasz and Aidan worked on designing, implementing and experimenting with different model variants for this work while at Google Brain and Google Research. However, their full names or roles are not clearly specified in the given context.   \n",
       "2  The paper introduces the Transformer, a new neural network architecture for sequence modeling tasks like machine translation that does not use recurrent connections. Instead, it relies entirely on attention mechanisms to capture long-range dependencies within the input and output sequences. The key motivations seem to be achieving better parallelization than recurrent models, while benefiting from the ability of attention to directly model relationships between distant elements in the sequences.   \n",
       "\n",
       "   faithfulness  \n",
       "0      1.000000  \n",
       "1      0.833333  \n",
       "2      1.000000  "
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from ragas import evaluate\n",
    "from ragas.metrics import (\n",
    "    faithfulness,\n",
    "    answer_relevancy,\n",
    "    context_recall,\n",
    "    context_precision,\n",
    "    context_entity_recall,\n",
    "    answer_similarity,\n",
    "    answer_correctness\n",
    ")\n",
    "\n",
    "\n",
    "#specify the metrics here\n",
    "metrics = [\n",
    "        faithfulness,       \n",
    "#         context_precision,\n",
    "#         context_recall\n",
    "#         context_entity_recall,\n",
    "#         answer_similarity,\n",
    "#         answer_correctness,\n",
    "#         harmfulness, \n",
    "#         maliciousness, \n",
    "#         coherence, \n",
    "#         correctness, \n",
    "#         conciseness\n",
    "    ]\n",
    "\n",
    "#You can also choose a different model for evaluation\n",
    "llm_for_evaluation = BedrockChat(model_id=modelId, client=bedrock_client)\n",
    "bedrock_embeddings = BedrockEmbeddings(model_id=\"amazon.titan-embed-text-v2\",client=bedrock_client)\n",
    "\n",
    "result = evaluate(\n",
    "    dataset = dataset, \n",
    "    metrics=metrics,\n",
    "    llm=llm_for_evaluation,\n",
    "    embeddings=bedrock_embeddings,\n",
    ")\n",
    "\n",
    "df = result.to_pandas()\n",
    "\n",
    "\n",
    "pd.options.display.max_colwidth = 800\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "You have now experimented with `RAGAS` SDK to evaluate a RAG Application using Anthropic Claude 3 as judge\n",
    "\n",
    "\n",
    "## Thank You"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
